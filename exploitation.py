from pyspark.sql import SparkSession

# Create a Spark session
spark = SparkSession.builder \
    .config("spark.jars", "./postgresql-42.6.0.jar") \
    .appName("Parquet to PostgreSQL") \
    .getOrCreate()

# Parquet files in HDFS
parquet_path1 = "hdfs://10.4.41.64:27000/user/bdm/formatted/idealista/idealista_year_district"
parquet_path2 = "hdfs://10.4.41.64:27000/user/bdm/formatted/price/price_year_district"
parquet_path3 = "hdfs://10.4.41.64:27000/user/bdm/formatted/income/income_year_district"

# Read Parquet files into DataFrames
df1 = spark.read.parquet(parquet_path1)
df2 = spark.read.parquet(parquet_path2)
df3 = spark.read.parquet(parquet_path3)

# Configure the PostgreSQL connection properties
jdbc_url = "jdbc:postgresql://10.4.41.64:5432/bdmdb"
connection_properties = {
    "user": "bdm",
    "password": "bdm",
    "driver": "org.postgresql.Driver"
}

# Write DataFrames to PostgreSQL
df1.write \
    .jdbc(url=jdbc_url, table="idealista", mode="append", properties=connection_properties)
df2.write \
    .jdbc(url=jdbc_url, table="price", mode="append", properties=connection_properties)
df3.write \
    .jdbc(url=jdbc_url, table="income", mode="append", properties=connection_properties)

# Stop the Spark session
spark.stop()
